# 4. Út a használható neurális hálózatos fordításig #

Az első részben bemutattam a neurális hálózatok kutatásának korai időszakát. A másodikban a backpropagation betanítási algoritmusról volt szó. A harmadik részben pedig a visszacsatolásos hálózatokról beszéltem. Most bemutatom, milyen út vezetett a használható neurális hálózat alapú fordításokig.

Nem térek ki olyan megoldásokra, melyek kevéssé járultak hozzá a Transformer architektúra kialakításához, de azért jó tudni, hogy volt itt még sok minden más is. Például képfeldolgozások esetén, az emberi látás agyi mechanizmusát leutánozva sikeresen alkalmazták a konvolúciós hálózatokat. A szövegekkel kapcsolatos feladatok, mint például a fordítás esetén azonban ezeknek nem vették hasznát.

Érdemes belegondolni, hogy a 80-as években a többség számára egyáltalán nem tűnt elképzelhetőnek, hogy egy neurális hálózat képes lehet megoldani egy olyan feladatot, mint amit most a ChatGPT esetén látunk. Maga az ötlet sem olyan triviális, hogy a gép folytasson, vagy generáljon a semmiből egy szöveget. Szerintem a legtöbben pár hónappal ezelőtt még nem is gondoltuk, hogy ilyen célra program készíthető. Szövegek esetén az egyik leggyakoribb felmerülő feladat a fordítás. Úgyhogy nem meglepő, hogy eleinte legtöbbször ezzel próbálkoztak.

A neurális hálózat alapú fordítás a 60-as években gyakorlatilag kudarcba fulladt, nem működött. Ehelyett a hagyományos algoritmusok használata kapott lendületet, ahol szavankénti fordítás után minden egyes nyelvtani szabályt vagy kivételt le kellett programozni. Ez nem volt teljesen használhatatlan, de egyre jobban látszott, hogy ezzel a módszerrel még évszázadokon keresztül kellene írni egy programot, mire minden kifejezést és esetet megfelelően kezelni tud. De egy akkora programot már nyilván a programozók se volnának képesek átlátni.

Nagyot lendített a gépi fordításon a statisztikai jellegű algoritmusok elterjedése. Ilyenkor óriási szövegkorpusz elemzése alapján, szavak bonyolult kategóriákba szervezésével és ügyes keresési megoldásokkal operálnak. Ezzel a módszerrel viszonylag pontos fordításokat sikerült a 90-es, 2000-es években produkálni, bár a fordítás tele volt hibával, félreértésekkel. A többértelmű esetekkel kevésbé boldogult, nem tudta megfelelően kihasználni a szövegkörnyezet segítségét, tehát nem értette a kontextust, hogy pontosan miről van szó. És stilárisan is elég béna mondatok jöttek létre.

Neurális hálózatokat eleinte a statisztikai rendszerek felturbózására vetettek be. Bizonyos kategorizálásokat végeztettek el velük, vagy több féle fordítási lehetőség közül segítettek választani ezek a módszerek.

A teljesen neurális hálózat alapú fordítás lehetőségében a legtöbben nem is igazán hittek. Pedig egyébként egyszerűen el lehetne képzelni egy ilyet. Képzeljünk el egy neuronhálót, ahol a bemenetre beadunk egy szöveget. Mondjuk betűnként, minden betű ASCII kódjával, tehát bemegy egy számsorozat, a mondat egészben az egyik nyelven. És a hálózatnak meg kellene tanulnia, hogy a kimenet adjon ki egy másik mondatot, a másik nyelvre való fordítást. Hát elképzelni könnyű, de ez a gyakorlatban nem működik. Talán egy iszonyatosan gigantikus hálózat képes lehetne erre is, de ettől még ma is messze járunk. Az biztos, hogy nem hatékony így csinálni.

De mi a probléma ezzel? Miért nehéz ez egy neurális hálónak, és miért tud megoldani más jellegű dolgokat? A probléma gyökerét leginkább Yoshua Bengio és kutatótársai kapargatták meg. Az a gond ezzel, hogy egy neurális hálózat nem szereti az ennyire konkrét értékeket. A diszkrét, egymástól ugrásszerűen különböző értékek helyett jobban boldogul a folyamatosakkal. Ha 65 helyett 66 lesz az eredmény, akkor már A helyett B lesz. Ha mindegyik szám csak egy kicsit megy mellé, semmi értelmes eredményt sem fogunk kapni. De maga a bemenet értelmezése sem egyszerű. A különféle bemenetek teljesen eltérőek. Akkor is egy teljesen más számot kap a rendszer, ha egyébként lenne valami hasonlóság egy másik értékhez. Mondjuk a nagy "A" és a kis "a" 65, illetve 97 ASCII kódolásban. Még ha a szöveget mondjuk kisbetűsítenénk, hogy ennek már nincs jelentősége, akkor is az van, hogy az egyes betűk közt nincs hasonlóság jelentésben. Nem tud a rendszer általános következtetéseket levonni. Nincs a bemenet szerkezetében semmi, ami a jelentés megértését segítené.

Bengio és társai 2001-ben és 2003-ba publikálták tanulmányaikat, ahol egy aprónak tűnő ötlettel kísérleteztek. A szöveget szavakra bontva táplálták a neuronhálózatba, de nem egyetlen számként, hanem minden szó egy hosszú számsorozatból, vagyis vektorból állt. Ezen belül a legegyszerűbb lehetőséget választották, vagyis a számok csupán 0-ák vagy 1-ek lehettek, és minden szó csak egyetlen 1-est tartalmazott, az összes többi érték 0 volt. Összeállítottak egy közepes méretű szótárat, a leggyakoribb pár ezer szóval. Ezeket ismerte csak a rendszer. Minden szónak egy olyan hosszú vektora volt, ahány féle szó szerepelt a szótárban. Az első szónak az első pozíción volt 1-ese, a többi 0. A második szónak a második pozíción, és így tovább.

A szavakat nem véletlenszerűen helyezték el a szótárban, hanem valamilyen jelentéssel összefüggő rendszer szerint sorba rendezve szerepeltek. Már amennyire ez sikerült. De ez a megoldás nem csak a bemeneti oldalon volt hasznos, hanem a kimeneten is. A hálózat által produkált válasz ugyanis nem egy konkrét szó vektora volt, tehát nem egy olyan eset, hogy mindenhol 0 jött, és csak egyetlen helyen 1-es. Elég nehéz is lenne összehozni ezt egy neurális háló szorzóit állítgatva. A kimenet minden egyes száma többnyire nullától különböző értéket vett fel. De az eredményt lehetett úgy értelmezni, hogy az a hálózat válasza, amelyik pozíción a legnagyobb érték szerepelt. Ezzel tehát kevésbé szigorú, kevésbé szögletes használati mód vált lehetővé. És további előnye volt, hogy nem csak egyetlen szó lehetett a válasz, hiszen így a kimenet tulajdonképpen azt is megmondja, hogy mely szó mennyire jó válasz. Lehet választani több, hasonlóan jó lehetőség közül.

2003 - Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin: A neural probabilistic language model
Université de Montréal (Canada)
https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
Hivatkozik saját korábbi munkájára 2000-ből, Hinton-ra és erre: "The idea of using neural networks for language modeling is not new either (e.g. Miikkulainen and Dyer, 1991)"

A módszert először részfeladatok elvégzésére használták. Például beszédfelismerés esetén, mikor már az akusztikai információ feldolgozásra került egy másfajta rendszer által, amely több lehetőséget talált, hogy vajon mi lehet a hallott kifejezés. A neurális hálózatnak ezen lehetőségeket kellett pontoznia, hogy mennyire értelmesek, így javítva az eredményen.

Fordításra a legelső megvalósítást 2012-ben publikálta Holger Schwenk. Itt a lehető legegyszerűbb előrecsatolásos hálózattal dolgoztak. Beadtak egy mondatot a bemenetre, és a hálózat kimenete kiadta a lefordított mondatot. A mondat maximum hét szóból állhatott. (Ha kevesebb szóra volt szükség, csupa nullákat kellett beadni.) A szótárban 16 ezer különböző szó volt, tehát hétszer ennyi bemenő értékkel kellett dolgozni. A betanítást 7 millió angol-francia fordítási példával végezték.

Publikáció: Holger Schwenk, 2012: https://aclanthology.org/C12-2104.pdf (Continuous Space Translation Models for Phrase-Based Statistical Machine Translation)(A publikációban említi, hogy a szoftverük egy nyílt forráskódú CSLM implementáció. Tehát volt egy korábbi CSLM nevű toolkit, és ők azt alakították át saját változatukra. A saját toolkitjüket 2010-ben pulbikálták már, de a tanulmányban megadott url már nem működik. (http://wwww.lium.univ-lemans.fr/~cslm)Viszont a GitHub-on ott van egy cuccos, Holger Schwenk egyetlen repója: Continuous Space Translation and Language Model (CSLM): https://github.com/hschwenk/cslm-toolkitA Readme tartalmazza is a történetet, tehát ennek első változata 2010-es: https://github.com/hschwenk/cslm-toolkit/blob/master/READMEA Readme négy tanulmányt említ, az első 2007-es. Itt is már ezt a módszert használták, de ott hangos szöveg felismerésére.)

Jól látszik, hogy ez a megoldás nem tökéletes. Egyrészt a bemeneti módszer nagyon pazarló, mert egy hosszú vektorban kevés információt tárolunk csupán el, egyetlen 1-est. Másrészt a szavak közti hasonlóság egydimenziós, tehát az összes létező szót egyetlen hosszú láncba kellene fűznünk, ami nyilván nem sikerülhet igazán. A fix hosszúság is gyenge pont, hogy fixen 7 szóról 7 szóra fordítunk. És a használható szavak mennyisége is kötött. Nyilván nem születtek csodaszép fordítások. De mégis, majdnem 70 év kutatás után ez volt az első alkalom, hogy önmagában egy neurális hálózat képessé vált fordításra. Anélkül, hogy szótárban keresgélne. Utólag ez talán nem tűnik olyan nagy dolognak, mert tudjuk, hogy megvalósítható. Ráadásul a megoldás lényegében nagyon egyszerű műveletek sorozata. Utólag pár hét alatt meg lehet csinálni, ha tudjuk, hogyan kell. De rátalálni erre, nem volt egyszerű.

A következő években tovább javítottak a módszeren. Egy fontos lépést 2013-ban publikálták Tomas Mikolov és társai, akik a szavakhoz tartozó vektorba több információt tettek be. Ezt az információt egy nagy szövegkorpuszon betanított kétrétegű neuronhálózattal állították elő. A módszer neve Word2Vec, vagy szó átalakítása vektorrá. A betanítás során a hálózat bemenetére került egy szöveg, amiből kivettek egy szót. (A korábbi, csak nullákat és 1-et tartalmazó kódolással). A hálózatnak ezt a hiányzó szót kellett kitalálnia, vagyis a paraméterek hangolása során ettől a hiányzó szótól való eltérést vették alapul. Miután nagy szövegkorpuszon betanították a hálózatot, egy-egy szót beadva kiolvasták a kimenetet, és az a vektor lett annak a szónak a reprezentációja. A vizsgálatok során azt találták, hogy a hasonló szavaknak hasonló a vektora. És ha a korábbi csak 0-ákat és 1-et tartalmazó vektor helyett ilyeneket használnak egy hálózat betanítására, sokkal jobb eredményt kapnak. Egyszerűen a rendszer több információt kap egy-egy szóról, nem csak annyit, hogy hanyadik volt a szótárban.https://en.wikipedia.org/wiki/Word2vec#:~:text=Word2vec%20is%20a%20technique%20for,words%20for%20a%20partial%20sentence.Mikolovék publikációja, 2013: https://www.researchgate.net/publication/234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space

Ezeket a módszereket word embedding-nek, vagyis szó-beágyazásnak hívják, ahol egy szót egy ahhoz tartozó vektorral írnak le.

A későbbi architektúrák esetén nem Mikolov Word2Vec megoldását használták már, de végeredményben ahhoz hasonló vektorokkal dolgoznak.

A Word2Vec jelentősége azonban ennél is nagyobb. Megnyitotta ugyanis az utat a generatív AI felé, amikor a feladat nem fordítás, hanem szöveg generálása, folytatása. Ugyanis ahogy láttuk, itt egy nagy szövegkorpuszon haladt végig a feldolgozás, amiből mindig mutattak egy részletet, egy szót kivéve. Ez a szó lehet a közepén, de akár a végén is, és akkor tulajdonképpen a rendszer a következő szót kell megtippelje. Voltak már hasonló próbálkozások korábban is, felmerült például, hogy ezt tömörítésre használják. Tehát egy algoritmus kell, ami megtippeli a következő elemet. Ha elég nagy arányban meg tudja tippelni, akkor nem kell letárolni a következőt. Csak akkor, ha a tippelés rossz eredményt adna.

Ennek a megoldásnak a másik hatalmas előnye, hogy a betanításhoz használt szöveget nem kell emberi munkával előállítani, vagy átnézni. Egyszerűen óriási szövegek kellenek, amikből automatikusan ki lehet venni, hogy mi a bemenet, és mi az elvárt kimenet. A rendszer tehát emberi beavatkozás nélkül maga képes tanulni. Nem kell mondjuk mondatpárokat keresni, ahogy az fordítás esetén szükséges. Mivel eddigre a hatalmas mennyiségű szövegekhez könnyű volt hozzáférni, szinte tárva volt az ajtó a generatív mesterséges intelligencia fejlődése előtt. De volt még egy nagy probléma, amit ehhez meg kellett oldani. Mégpedig az, hogy hogyan lépjük át a fix méret problémáját. Schwenk megoldása ugyanis 7 szó bemenetet tudott kezelni, amiből 7 szót generált. Hogy ez most fordítás, vagy 7 újabb szó, az mindegy. De ez a fix méretbeli kötöttség nem túl jó.

A következő részben megmutatom, mi volt erre a megoldás.