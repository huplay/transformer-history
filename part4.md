# 4. Út a használható neurális hálózatos fordításig #

Az eddigi részekben bemutattam a neurális hálózatok kutatásának korai időszakát, majd a backpropagation betanítási algoritmust és a visszacsatolásos hálózatokat. Most bemutatom, milyen út vezetett az első neurális hálózattal megvalósított fordításokig.

Érdemes belegondolni, hogy a 80-as években a többség számára egyáltalán nem tűnt elképzelhetőnek, hogy egy neurális hálózat képes lehet megoldani egy olyan feladatot, mint amit most a ChatGPT esetén látunk. Maga az ötlet sem olyan triviális, hogy a gép folytasson, vagy generáljon a semmiből egy szöveget. (A ChatGPT közismertté válása előtt a legtöbb embernek eszébe sem jutott, hogy ilyen célra egyáltalán program készíthető.) Léteztek ugyan korai csetelő programok is, melyek hagyományos algoritmussal próbáltak beszélgetést imitálni, de nem magától értetődő erre úgy gondolni, hogy a program szöveget folytat. Ráadásul ezek kudarca inkább csak arra ébresztette rá a fejlesztőket, hogy mennyire reménytelen egy ilyen megvalósítása. Egy beszélgetés során olyan komplex módon reagálunk, annyi minden hatással van válaszunkra, hogy úgy tűnt, az ilyesmi feladat az egyik utolsó lépés lesz az általános emberi intelligenciához elvezető úton. 

A kezdeti időkben a legkézenfekvőbb szöveges gépi feladat a fordítás volt, de ennek neurális hálózatos megvalósítása a 60-as években gyakorlatilag kudarcba fulladt; nem működött. Ehelyett a hagyományos algoritmusok használata kapott lendületet, ahol szavankénti fordítás után minden egyes nyelvtani szabályt vagy kivételt le kellett programozni. Ez nem volt teljesen használhatatlan, de egyre jobban látszott, hogy így még évszázadokon át írhatnánk a programot, mire minden kifejezést és esetet megfelelően kezelni tud. De egy akkora kódot már a programozók sem volnának képesek átlátni.

Nagyot lendített a gépi fordításon a statisztikai jellegű algoritmusok elterjedése, ahol óriási szövegkorpusz elemzése alapján, szavak kategóriákba szervezésével és speciális keresési megoldásokkal operáltak. Ezzel a módszerrel viszonylag pontos fordításokat sikerült a 90-es, 2000-es években produkálni, bár a fordítás tele volt hibával, félreértésekkel. A többértelmű esetekkel kevésbé boldogult, nem tudta megfelelően kihasználni a szövegkörnyezet segítségét, tehát nem értette a kontextust, hogy pontosan miről van szó. Stiláris szempontból is gyenge mondatok jöttek létre.

Neurális hálózatokat eleinte a statisztikai rendszerek felturbózására vetettek be. Bizonyos kategorizálásokat végeztettek el velük, vagy fordítási lehetőségek közti választásban segítettek.

A teljesen neurális hálózat alapú fordítás lehetőségében a legtöbben nem is igazán hittek. Pedig milyen egyszerű elképzelni egy ilyet! Vegyünk egy neurális hálót, ahol a bemenetre beadunk egy szöveget. Mondjuk betűnként, minden betű ASCII kódjával. Tehát bemegy egy számsorozat, a mondat egészben az egyik nyelven, és a hálózatnak meg kellene tanulnia, hogy a kimenet adjon ki egy másik mondatot, a fordítást. Hát elképzelni könnyű, de a gyakorlatban nem működik. Talán egy hihetetlenül gigantikus hálózat képes lehet erre is, de ettől még ma is messze járunk. Az biztos, hogy nem hatékony így csinálni.

De miért van ez így? Miért nehéz ez egy neurális hálónak, és miért tud megoldani más jellegű dolgokat? A probléma gyökerét leginkább Yoshua Bengio és kutatótársai kapargatták meg. Az a gond itt, hogy egy neurális hálózat nem szereti az ennyire konkrét értékeket. A diszkrét, egymástól ugrásszerűen különböző értékek helyett jobban boldogul a folyamatosakkal. Ha 65 helyett 66 lesz az eredmény, az A helyett B-t jelent. Ha mindegyik szám csak pár értékkel megy mellé, rendszertelenül, nem kapunk értelmes eredményt. De a feldolgozandó adathalmazból az információ kinyerése sem egyszerű. A különféle bemenetek teljesen eltérőek. Akkor is egy teljesen más számot kap a rendszer, ha egyébként lenne valami hasonlóság egy másik értékhez. Mondjuk a nagy "A" és a kis "a" 65, illetve 97 ASCII kódolásban. Még ha a szöveget mondjuk kisbetűsítenénk, hogy ennek már nincs jelentősége, akkor is a helyzet, hogy az egyes közeli betűk közt nincs hasonlóság jelentésben. Nem tud a rendszer elvonatkoztatottabb következtetéseket levonni. Nincs a bemenet szerkezetében semmi, ami a jelentés megértését segítené.

Bengio és társai 2001-ben és 2003-ba publikálták tanulmányaikat, ahol egy aprónak tűnő ötlettel kísérleteztek. A szöveget szavakra bontva táplálták a neuronhálózatba, de szavanként nem egyetlen számként, hanem minden szó egy hosszú számsorozatból, vagyis vektorból állt. Ezen belül a legegyszerűbb lehetőséget választották, vagyis a számok csupán 0-ák vagy 1-ek lehettek, és minden szó csak egyetlen 1-est tartalmazott. (Az összes többi érték 0 volt.) Összeállítottak egy közepes méretű szótárat, a leggyakoribb pár ezer szóval. Ezeket ismerte csak a rendszer. Minden szónak egy olyan hosszú vektora volt, ahány szó szerepelt a szótárban. Az első szónak az első pozíción volt 1-ese, a többi 0. A második szónak a második pozíción, és így tovább. (Ezt 1-of-N embeddingnek nevezik.)

| <img src="images/YoshuaBengio.jpg" height="300" /> | <img src="images/WordEmbeddingVectors.png" height="300" />  |
|:--------------------------------------------------:|:-----------------------------------------------------------:|
|                   Yoshua Bengio                    | Szavak vektorokká kódolása (Itt csak 8 szó van a szótárban) |

A szavakat nem véletlenszerűen helyezték el a szótárban, hanem valamilyen jelentéssel összefüggő rendszer szerint sorba rendezve szerepeltek. Már amennyire ez sikerült - nyilván nem tökéletesen, de a szavak szótáron belüli pozíciója adhatott valami segítséget a hálózatnak. A megoldás nem csak a jelentés jobb megragadásában, vagyis a bemeneti oldalon volt előnyös, hanem a kimeneten is. A hálózat által produkált válasz ugyanis nem egy konkrét szó vektora volt, tehát nem egy olyan eset, hogy mindenhol 0 jött, csak egyetlen helyen 1-es. (Elég nehéz is lenne mindig ilyen tökéletes válaszokat összehozni csupán a neurális háló szorzóit állítgatva.) A kimenet minden egyes száma többnyire nullától különböző értéket vett fel. De ahol a legnagyobb érték szerepel, az tekinthető 1-nek, a többi pedig a 0, vagyis az így leírt szó lesz a válasz. Ezzel kevésbé szigorú, kevésbé szögletes használati mód vált lehetővé, ki lett simítva, folyamatosabbá lett téve mind a bemenet, mind a kimenet. Ilyen körülmények közt tud jobban boldogulni egy neurális háló.

További előny, hogy nem csak egyetlen szó lehet a válasz, hiszen így a kimenet tulajdonképpen azt is megmondja, hogy mely szó mennyire jó válasz. Lehet választani több, hasonlóan jó lehetőség közül.

A módszert először részfeladatok elvégzésére használták. Például beszédfelismerés esetén, mikor már az akusztikai információ feldolgozásra került egy másfajta rendszer által, amely több lehetőséget talált, hogy vajon mi lehet a hallott kifejezés. A neurális hálózatnak ezen lehetőségeket kellett pontoznia, hogy mennyire értelmesek, így javítva az eredményen.

Fordításra a legelső megvalósítást 2012-ben publikálta Holger Schwenk. Itt a lehető legegyszerűbb előrecsatolásos hálózattal dolgoztak. Beadtak egy mondatot a bemenetre, és a hálózat kimenete kiadta a lefordított mondatot. A mondat maximum hét szóból állhatott. (Ha kevesebb szóra volt szükség, csupa nullákat kellett beadni.) A szótárba 16 ezer szót tettek, tehát minden szót egy ilyen hosszú számsorozat írt le. A hét szó hosszúsághoz hétszer ennyi bemenő neuronra volt szükségük. A tanítást 7 millió angol-francia fordítási példával végezték.

| <img src="images/HolgerSchwenk.png" height="300" /> |
|:---------------------------------------------------:|
|                   Holger Schwenk                    |

Tehát egy egyszerű neurális háló, plusz Bengio apró ötlete, hogy a szavakat 0-ákat és 1-eket tartalmazó vektorral kódolják el, máris egy működő fordítóprogramot eredményezett. Bizonyára nem volt ez tökéletes, de nagyjából megtalálta a megfelelő szavakat. Nem mondom, hogy ezt akár a 60-as években is megcsinálhatták volna, mert akkor bizonyára nem állt még rendelkezésre kellő számítási kapacitás, és nyilván 7 millió fordítás sem volt meg digitálisan. De a 90-es években valószínűleg már nem lett volna kivitelezhetetlen. Mégis, kellett még durván két évtized, mire idáig eljutottak, csak azért, mert hiányzott egy apró ötlet.

Persze azt is könnyű belátni, hogy ez a megoldás nem tökéletes. Az egyik gyenge pont a kötött hosszúság, vagyis hogy fixen hét szóról hét szóra fordítunk. A használható szavak száma is kötött, olyan mondatot nem tud lefordítani, amiben ismeretlen szó van. Ráadásul a bemeneti módszer nagyon pazarló, mert egy hosszú számsorozatban kevés információt tárolunk, csupán egyetlen 1-est. Mindemellett a hálózat hatékonyságát rontja, hogy ugyan elvileg a hasonló szavak egymás mellé kerültek, de ezt nem lehet tökéletesen megvalósítani, mert a szavak közti hasonlóság nem egydimenziós. Nem lehet az összes szót egyetlen hosszú láncba fűzni, ahol minden elem valamilyen jelentéssel összefüggő módon hasonlít a szomszédjához.

Az utóbbi problémán többen is dolgozni kezdtek, de nagyjából egy évtized kellett, mire rátaláltak az első igazán jó megoldásra. Az nyilvánvaló, hogy valahogy több információt kell betenni a szavakhoz tartozó vektorba. Csak az a kérdés, hogy pontosan micsodát. Tomas Mikolov és társai 2013-ban publikálták a Word2Vec módszert, ami előállította ezeket a vektorokat. Kétrétegű neuronhálózatot használtak, amit nagy szövegkorpuszon tanítottak be. A bemenetre tett szövegrészletből mindig kihagytak egy szót, és az a hiányzó szó volt az elvárt kimenet, amitől való eltérés alapján hangoltak a paramétereken. (Természetesen backpropagation megoldással.) A tanítás során minden szót a Bengio-ék által kitalált módszerrel, vagyis a csak nullákat és 1-et tartalmazó kódolással táplálták be. De miután véget ért a betanítás, megnézték, hogy milyen kimeneti vektort ad a rendszer minden egyes szó esetén. És ezek a kimeneti értékek kerültek be egy új szótárba.

Így tulajdonképpen a szavak egy többdimenziós térben lettek elhelyezve. (Annyi dimenziós, ahány számból áll a vektor.) A vizsgálatok során azt találták, hogy a hasonló szavak ebben a sokdimenziós vektortérben egymáshoz közel helyezkednek el. (Három dimenzió esetén ezt még el is tudjuk képzelni, ha három elemű lenne a vektor. Egy 3D-s koordinátarendszerben, vagyis itt a térben a hasonló szavak közel lesznek egymáshoz. Csak itt több száz számról, több száz vagy több ezer dimenzióról van szó.)

Ezt a sokdimenziós vektoros módszert hívják word embeddingnek, vagyis szó-beágyazásnak. (Mondhatjuk, hogy a Bengio féle, 0-ákat és 1-et tartalmazó vektor is az volt, csak a legegyszerűbb esete.)

Ha egy neuronhálózatot úgy tanítanak be, hogy a bemenetre ezen a módon elkódolva tesszük a szöveget, a hálózat sokkal könnyebben meg tudja tanulni a számára kitűzött feladatot. Több információt kap egy-egy szóról, nem csak annyit, hogy hanyadik volt a szótárban. A mai architektúrák esetén nem Mikolov Word2Vec megoldását használják már, de végeredményben ahhoz hasonló vektorokkal dolgoznak.

| <img src="images/TomasMikolov.png" height="300" /> |
|:--------------------------------------------------:|
|                   Tomáš Mikolov                    |

A Word2Vec jelentősége túlmutat a hálózat hatékonyságának növelésén. Megnyitotta az utat a generatív mesterséges intelligencia felé, mikor a feladat nem fordítás, hanem szöveg generálása, folytatása. Ugyanis ahogy láttuk, itt egy nagy szövegkorpuszon haladt végig a feldolgozás, amiből mindig mutattak egy részletet, egyetlen szót kivéve. Ez a szó lehet a közepén, de akár a szövegrész végén is, és utóbbi esetben tulajdonképpen a rendszer a következő szót kell megtippelje.

Voltak már hasonló próbálkozások korábban is, felmerült például, hogy ilyesmit tömörítésre használjanak. Tehát egy algoritmus kell, ami megtippeli a következő elemet. Ha elég nagy arányban meg tudja tippelni, akkor nem kell letárolni a következő elemet, mert kiolvasáskor is megtippelhető. Csak akkor kell plusz információ, ha tudjuk, hogy az algoritmus rosszul tippel valahol. Tehát Mikolovék ötlete sem előzmények nélküli, de mindenképpen nagy hatással volt a nyelvi feladatokon dolgozó kutatók gondolkodására. 

Az ilyen jellegű betanításnak a másik hatalmas előnye, hogy a szöveget nem kell emberi munkával előállítani, vagy átnézni. Egyszerűen óriási szövegmennyiség kell, amiből automatikusan ki lehet venni, hogy mi a bemenet, és mi az elvárt kimenet. A rendszer tehát emberi beavatkozás nélkül maga képes tanulni. Nem kell mondjuk mondatpárokat keresni, ahogy az fordítás esetén szükséges. Mivel eddigre hatalmas mennyiségű digitális szöveghez lehetett hozzáférni, tárva volt az ajtó a generatív mesterséges intelligencia fejlődése előtt.

Végezetül megemlítek egy kisebb jelentőségű újítást, ami a fix méretű szótár problémáját orvosolta. Bármilyen sok szót is teszünk a szótárunkba, biztos lesznek hiányzóak. Megoldható ugyan, hogy betűnként adagoljuk a szöveget (és így azokból bármi összeállítható), ez azonban nem előnyös, mert így a neuronhálózat alig tudja kikövetkeztetni a szövegben rejlő tartalmi rendszert. Részmegoldás, ha irgalmatlanul nagy, több százezer szót tartalmazó szótárral dolgozunk. (Ezt alkalmazzák is, de még így is maradnak ismeretlen szavak. Főleg, ha soknyelvű a rendszer.) Az eddigi legjobb megoldás a byte-pair encoding, ami a szavak és a betűk közt félúton áll, vagyis szavak és szó-részletek, valamint betűk vegyesen reprezentáltak. (2015, Sennrich, Haddow, Birch.) (A byte-pair encoding (BPE) eredetileg tömörítési eljárás volt.)
