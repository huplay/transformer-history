# A legfontosabb publikációk #

## 1940-es évek ##

- Perceptron (mesterséges neuron első elméleti modellje)
  - `1943`
  - Warren S. McCulloch, Walter Pitts
  - A logical calculus of the ideas immanent in nervous activity
  - https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf


- Az első tanítási algoritmus (Hebb-módszer)
  - `1949`
  - D. O. Hebb
  - The organization of behavior - A neuropsychological theory
  - https://pure.mpg.de/rest/items/item_2346268_3/component/file_2346267/content


## 1950-es évek ##

- Perceptron első széles körben ismertté vált megvalósítása
  - `1958`
  - Frank Rosenblatt
  - The perceptron - A probabilistic model for information storage and organization in the brain
  - https://www.academia.edu/60542953/The_perceptron_a_probabilistic_model_for_information_storage_and_organization_in_the_brain

  
## 1960-es évek ##

- RELU aktivációs függvény
  - `1969`
  - Kunihiko Fukushima


## 1970-es évek ##

- A backpropagation tanuló algoritmus első leírása
  - `1974`
  - Paul John Werbos
  - Beyond regression - New tools for prediction and analysis in the behavioral science (Ph. D. Thesis)
  - https://www.researchgate.net/publication/279233597_Beyond_Regression_New_Tools_for_Prediction_and_Analysis_in_the_Behavioral_Science_Thesis_Ph_D_Appl_Math_Harvard_University


## 1980-es évek ##

- A backpropagadion tanuló algoritmus újbóli felfedezése
  - `1985`
  - Yann le Cun
  - A theoretical framework for back propagation
  - Egy későbbi, angol nyelvű publikáció (az eredeti francia): https://www.researchgate.net/publication/2360531_A_Theoretical_Framework_for_Back-Propagation


- A backpropagation tanuló algoritmus másik újbóli felfedezése
  - `1985`
  - David Parker


- A backpropagation tanuló algoritmus közismertté tétele
  - `1986`
  - David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams
  - Learning representations by back-propagating errors
  - https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769


- A közönséges visszacsatolásos hálózat első publikálása, a backpropagation algoritmus alkalmazásával együtt
  - `1986`
  - Michael I. Jordan
  - Serial order - A parallel distributed processing approach
  - https://cseweb.ucsd.edu/~gary/PAPER-SUGGESTIONS/Jordan-TR-8604-OCRed.pdf


## 1990-es évek ##

- Fordítás visszacsatolásos neurális hálózattal
  - `1997` 
  - Ramon P. Neco, Mikel L. Forcada
  - Asynchronous translations with recurrent neural nets
  - https://ieeexplore.ieee.org/document/614693
  - https://www.cs.brandeis.edu/~gregws/cs113/asynchronous-translations-with-recurrent.pdf

## 2000-es évek ##

- A bemenet és kimenet vektorként való reprezentálása (1-of-N embedding)
  - `2003`
  - Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin
  - A neural probabilistic language model
  - https://jmlr.org/papers/volume3/tmp/bengio03a.pdf


- ImageNET, 2009

## 2010-es évek ##

- Fordítás előrecsatolt hálózattal, fix méretű bemenet és kimenet (7-7 szó), az első, ami 1-of-N embedding-et használt
  - `2012`
  - Holger Schwenk
  - Continuous space translation models for phrase-based statistical machine translation.pdf
  - https://aclanthology.org/C12-2104/


- AlexNET, 2012


- Az első enkóder-dekóder architektúra neurális háló alapú fordításra, 1-of-N embedding használatával
  - `2013. jún. 15.`
  - Nal Kalchbrenner, Phil Blunsom
  - Recurrent convolutional neural networks for discourse compositionality
  - https://arxiv.org/abs/1306.3584


- Word2Vec word embedding
  - `2013. jan. 16.`
  - Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean
  - Google
  - Efficient estimation of word representations in vector space
  - https://arxiv.org/abs/1301.3781


- Enkóder-dekóder architektúrájú fordítás-támogató rendszer (statisztikai rendszer részeként), a GRU létrehozása (LSTM leegyszerűsítésével)
  - `2014. jún. 3.`
  - Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio
  - Learning phrase representations using RNN encoder–decoder for statistical machine translation
  - https://arxiv.org/abs/1406.1078


- Az attention-mechanizmus létrehozása, melyet első szerzőjéről Bahdanau-attention-nek neveznek
  - `2014. szept. 1.`
  - Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio
  - Neural machine translation by jointly learning to align and translate
  - https://arxiv.org/abs/1409.0473


- Enkóder-dekóder architektúrájú, LSTM egységeket használó fordítás, amely megközelítette a legjobb statisztikai fordítók teljesítményét
  - `2014. szept. 10.`
  - Ilya Sutskever, Oriol Vinyals, Quoc V. Le
  - Sequence to sequence learning with neural networks
  - https://arxiv.org/abs/1409.3215


- Tokenizáció Byte-Pair Encoding használatával
  - `2015. aug. 31.`
  - Rico Sennrich, Barry Haddow, Alexandra Birch
  - Neural machine translation of rare words with subword units
  - https://arxiv.org/abs/1508.07909


- Intra-sentence attention (self-attention, de még visszacsatolásos hálózatokra)
  - `2016. jan. 25.`
  - Jianpeng Cheng, Li Dong, Mirella Lapata
  - Long Short-Term Memory-Networks for Machine Reading
  - https://arxiv.org/abs/1601.06733


- Self-attention előrecsatolásos hálózatokra (a Transformer architektúra korai változata. A párhuzamosítás még nincs megoldva, de az ötlet azt a célt szolgálta, hogy a párhuzamosítás lehetővé váljon.)
  - `2016. jún. 6.`
  - Ankur P. Parikh, Oscar Täckström, Dipanjan Das, Jakob Uszkoreit
  - Google Brain
  - A Decomposable Attenntion Model for Natural Language Inference
  - https://arxiv.org/abs/1606.01933


- Transformer architektúra
  - `2017. jún. 12.`
  - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin
  - Google Brain
  - Attention is all you need
  - https://arxiv.org/abs/1706.03762


- Csak dekóderes Transformer architektúra
  - `2018. jan. 30.`
  - Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Łukasz Kaiser, Noam Shazeer
  - Google Brain
  - Generating Wikipedia by summarizing long sentences
  - https://arxiv.org/abs/1801.10198


- GELU aktivációs függvény
  - `2016. jún. 27.`
  - Dan Hendrycks, Kevin Gimpel
  - Gaussian Error Linear Units (GELUs)
  - https://arxiv.org/abs/1606.08415


- GPT-1 - A csak dekóderes Transformer architektúra első teljesen nyilvánosságra hozott megvalósítása
  - `2018. jún. 11.`
  - Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever
  - OpenAI
  - Improving language understanding by generative pre-training
  - https://openai.com/blog/language-unsupervised/
  - https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
  - Source code: https://github.com/openai/finetune-transformer-lm


- GPT-2
  - `2019. febr. 14.`
  - Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever
  - OpenAI
  - Language models are unsupervised multitask learners
  - https://openai.com/blog/better-language-models/
  - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
  - Source code: https://github.com/openai/gpt-2


- Sparse transformer
  - `2019. ápr. 23.`
  - Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever
  - OpenAI
  - Generating long sequences with sparse transformers
  - https://openai.com/blog/sparse-transformer/
  - https://arxiv.org/pdf/1904.10509.pdf
  - Source code: https://github.com/openai/sparse_attention

## 2020-as évek ##

- GPT-3
  - `2020. máj. 28.`
  - Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei
  - OpenAI
  - Language Models are Few-Shot Learners
  - https://arxiv.org/abs/2005.14165
  - https://paperswithcode.com/paper/language-models-are-few-shot-learners/review/
  - Source code (not complete): https://github.com/openai/gpt-3


- InstructGPT


- Codex


- ChatGPT, 2022


- GPT-4, 2023
