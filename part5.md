# 5. Encoder-decoder architektúra és attention mechanizmus #

Az első részben bemutattam a neurális hálózatok kutatásának korai időszakát. A másodikban a backpropagation betanítási algoritmusról volt szó. A harmadik részben a visszacsatolásos hálózatokról beszéltem, a negyedikben pedig arról, hogy a word embedding trükkje miért vezetett el az első használható neurális fordításig. Azt is láttuk, hogy a fix méret kötöttsége tűnt a következő megoldandó feladatnak. Nézzük, hogyan valósult ez meg!

A problémát ekkor az enkóder-dekóder architektúra használatával orvosolták, melyet először Nal Kalchbrenner és Phil Blunsom valósított meg 2013-ban.

Itt is a Bengio által javasolt, 0-ákból és 1-ből álló vektort használták az egyes szavak reprezentálására. Az újdonság a visszacsatolásos architektúra megjelenése volt, amelynek működési alapja tulajdonképpen ugyanaz, mint ami a közönséges visszacsatolásos hálózat esetén történik. Ezt a harmadik részben mutattam be, de frissítsük fel egy picit emlékeinket! Tehát ott az történt, hogy a bemenetre kiadott választ a következő menetben újra beadtuk, a következő bemenet mellé téve. Ez felel meg az enkóder résznek, ahol pontosan ugyanez történik. Beadjuk a lefordítandó mondatot szavanként. Először a bemenet első fele üres, a második része tartalmazza az első szót. A hálózat által kiadott eredmény lesz a következő futtatás bemenetének első fele, a második felébe kerül a második szó. És így tovább a lefordítandó szöveg utolsó szaváig. Az utolsó szónál kapott kimenet lesz az enkóder végső kimenete, amibe tehát az összes szó eredménye bele van itatva.

A következő lépés ez alapján egy másik nyelvű szöveg előállítása. Ami majdnem ugyanez az algoritmus, csak egy másik neurális hálózaton, amit dekódernek neveztek. Ennek első bemenete az enkóder kimenete. Amit pedig kiad a dekóder, az lesz a következő bemenete. Egészen addig, míg azt nem mondja, hogy kész. (Van egy speciális szó, hogy vége a szövegnek.)

2013 - Nal Kalchbrenner, Phil Blunsom : New encoder-decoder architecture for machine translationConvolutional Neural Network (CNN) for encoder, Recurrent Neural Network (RNN) for decoderhttps://aclanthology.org/D13-1176/

Igaz, hogy az enkóder-dekóder architektúra felszabadít minket a fix méretek korlátja alól, de behoz egy újabb problémát, amit a közönséges visszacsatolásos neurális hálózat esetén láttunk. Vagyis azt, hogy a legelőször betáplát szó hatása az újabb és újabb menetekben egyre inkább kimosódik. Ezért aztán az enkóder-dekóder architektúrát nem sima előrecsatolásos hálózattal oldották meg, hanem olyannal, aminek önmagában is van némi memóriája, vagyis visszacsatolásos hálózattal. Tehát túl azon, hogy a kimenetet a következő futtatáskor újra betettük a bemenetre, a használt hálózat önmagában visszacsatolást tartalmazott. A fenti csapat konvolúciós hálózatot használt. Bengio és Schwenk másokkal együtt (Cho, Marrienboer, Gulchechre, Bahdanau, Bougares) szintén elkezdett egy enkóder-dekóder megoldáson dolgozni, amit 2014-ben publikáltak. Ők az LSTM-ből indultak ki, de hogy javítsanak a számítási teljesítményen, kifejlesztették annak egyszerűsített változatát, a GRU-t. (Ezt a megoldást nem önálló fordításra használták, hanem egy statisztikai rendszer részeként.) Ugyanebben az évben kicsit később Sutskever és társai is előálltak egy LSTM-et használó enkóder-dekóderes fordító megoldással, amely minden korábbinál jobb eredményt produkált.

2014 - Sutskever, Vinyals, Quoc: Sequence to Sequence Learning with Neural Networks, Googlehttps://arxiv.org/abs/1409.3215Presentáció (Sutskever): https://www.youtube.com/watch?v=-uyXE7dY5H0

A jobb eredményt Sutskever egy apró trükknek tulajdonította. Mégpedig annak, hogy a lefordítandó mondatot hátulról előre adták be a rendszernek, arra gondolva, hogy a mondat első szavai fontosabbak. Ugyanis számukra is nyilvánvaló volt a közönséges visszacsatolásos hálózatoknál említett probléma, a vanishing gradient. Vagyis az, hogy ha hosszabb mondatokat adunk be a hálózatnak, a korábban mutatott szavak hatása egyre jobban halványul, hamarosan teljesen kimosódik. Sutskeverék megoldásában a legutolsó szavak mosódtak ki hamarabb, az első szavak hatása nagyobb volt. Ez angol bemenő szöveg esetén valószínűleg tényleg segít, de a problémát nem szünteti meg, csak kicsit csökkenti.

Hogy mi az igazi probléma, azt megintcsak Bengio csapata találta meg. Bahnadauval és Cho-val együtt ők hárman még ugyanebben az évben, 2014-ben publikálták is rá a megoldást, az attention mechanizmust. De mi is az igazi probléma? Hát az, hogy hiába van bármilyen memóriája a neurális hálózatnak, a kimenete egy fix méretű vektor. Ha csak két szóra kell emlékeznie, vagy ha kétezerre, ugyanolyan hosszúságú vektor lesz az eredmény, amit aztán a dekódernek egy másik nyelvre kell fordítania. Nyilván nem férhet ebbe a fix méretű vektorba elég információ tetszőlegesen hosszú bemenő mondatról.

De hogyan segít ezen az attention mechanizmus? Úgy, hogy hagyományos memóriát használva minden egyes kimenetet eltárolunk. És a dekóder nem csak egyetlen, hanem az összes köztes eredményt is használhatja, amit az enkóder produkált. Már csak azt kell megoldani, hogy hogyan használja ezt, mert a dekódernél is fix méretű vektor van. A megoldás az, hogy a sok eltárolt kimenetet úgy kell egymásra helyezni (összeadni), hogy bizonyos szavak eredményét fontosabbnak tartjuk, nagyobb figyelmet szentelünk neki, mint másoknak.

Ezt az attention mechanizmust Bahnadau-attention-nek nevezik, mert Bengio és Cho előtt ő volt a tanulmány első szerzője. Lényegében megegyezik azzal, ami a Transformerben található. Ugyanúgy van query, key és value mátrix, ugyanolyan a pontozási megoldás. Épp csak a multihead rész hiányzik - de az úgyis csak azért van, hogy jobban párhuzamosítható legyen a feldolgozás.

Igen közel volt már ez a Transformer architektúrához. Az egyetlen lényeges különbség az volt, hogy odáig mindenki visszacsatolásos architektúrát, vagyis konvolúciós hálózatot, LSTM-et vagy GRU-t használt. 2017-ben azonban a Google Brain csapata észrevette, hogy sima előrecsatolásos hálózat esetén sokkal hatékonyabban elvégezhető a számítás, így jóval nagyobb hálózat építhető.

A következő, egyben utolsó részben bemutatok még néhány fejlesztést, ami hozzájárult a Transformer architektúra sikerességéhez, illetve röviden vázolom az legfontosabb megvalósításokat.